{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/anlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# RAG_System.ipynb\n",
    "\n",
    "# ============================\n",
    "# 1. Install Required Packages\n",
    "# ============================\n",
    "# You might already have some or all of these. If so, you can skip or comment them out.\n",
    "# %pip install langchain transformers chromadb sentence-transformers accelerate bitsandbytes  # etc.\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "import shutil\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import gc\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"mps\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 2. Configuration\n",
    "# ============================\n",
    "# Path to data folder\n",
    "TEXT_DATA_PATH = [\"../data/zianp\", \"../data/dunhanj\"] \n",
    "ROW_EVENT_PATH = ['../data/nicolaw']\n",
    "STATIC_WEB_CSV_PATH = '../data/texts_urls_filtered.csv'\n",
    "custom_cache_dir = \"/mnt/new_volume\"\n",
    "\n",
    "# Choose an embedding model.\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_INPUT = \"qwen2\"\n",
    "# Choose a local LLM model.\n",
    "# LLM_MODEL_ID = \"tiiuae/falcon-7b-instruct\"\n",
    "# LLM_MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "LLM_MODEL_MAP = {\"falcon\": \"tiiuae/falcon-7b-instruct\"\n",
    "                ,\"llama3\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "                ,\"deepseek-r1\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "                ,\"phi-4\": \"unsloth/phi-4-bnb-4bit\"\n",
    "                ,\"qwen2\": \"Qwen/Qwen2-7B-Instruct\"\n",
    "                ,\"qwq\":\"Qwen/QwQ-32B\"}\n",
    "\n",
    "LLM_MODEL_ID = LLM_MODEL_MAP[LLM_INPUT]\n",
    "# LLM_MODEL_ID = \"unsloth/phi-4-bnb-4bit\"\n",
    "\n",
    "LLM_NAME = LLM_MODEL_ID.split(\"/\")[-1]\n",
    "data_file = \"qa400\"\n",
    "test_data_path =\"../annotations/{}.csv\".format(data_file)\n",
    "\n",
    "retriever_top_k = 10\n",
    "CHUNK_SIZE = 512  \n",
    "CHUNK_OVERLAP = 100\n",
    "RELOAD_VECTORS_DB = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify files in the folder\n",
    "\n",
    "files_txt_path = []\n",
    "files_csv_path = []\n",
    "files_event_path = []\n",
    "\n",
    "for DATA_PATH in TEXT_DATA_PATH:\n",
    "    for root, dirs, files in os.walk(DATA_PATH):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                files_txt_path.append(os.path.join(root, file))\n",
    "            elif file.endswith('.csv'):\n",
    "                files_csv_path.append(os.path.join(root, file))\n",
    "\n",
    "for DATA_PATH in ROW_EVENT_PATH:\n",
    "    for root, dirs, files in os.walk(DATA_PATH):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                files_event_path.append(os.path.join(root, file))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8561 raw documents from 13 files.\n",
      "Total 99426 final chunks prepared for vector storage.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================\n",
    "# 2. Load Files with Different Strategies\n",
    "# ============================\n",
    "all_documents = []\n",
    "\n",
    "# Load Dunhan CSV\n",
    "test_df = pd.read_csv(STATIC_WEB_CSV_PATH)\n",
    "for index, row in test_df.iterrows():\n",
    "    \n",
    "    all_documents.append(Document(page_content=row['TEXT'], metadata={\"source\": row['URL']}))\n",
    "\n",
    "# Load all files in the directory\n",
    "for file_path in files_txt_path:\n",
    "    loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "    doc = loader.load()  # Load entire file as one document\n",
    "    all_documents.append(Document(page_content=doc[0].page_content, metadata={\"source\": file_path}))\n",
    "\n",
    "for file_path in files_csv_path:\n",
    "    df = pd.read_csv(file_path)\n",
    "    filename = os.path.basename(file_path)\n",
    "    for index, row in df.iterrows():\n",
    "        row_text = f\"{filename} | \" + \" | \".join(f\"{col}: {row[col]}\" for col in df.columns)\n",
    "        metadata = {\"source\": filename, \"row_id\": index}\n",
    "        all_documents.append(Document(page_content=row_text, metadata=metadata))\n",
    "\n",
    "\n",
    "# OPTIOANL function for processing files row by row\n",
    "    # ✅ Load row by row (structured data)\n",
    "for file_path in files_event_path:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for row_id, line in enumerate(file):\n",
    "            line = line.strip()\n",
    "            if line:  # Ignore empty lines\n",
    "                all_documents.append(Document(page_content=line, metadata={\"source\": filename, \"row_id\": row_id}))\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(all_documents)} raw documents from {len(os.listdir(DATA_PATH))} files.\")\n",
    "\n",
    "# ============================\n",
    "# 3. Split Longer Documents for Better Retrieval\n",
    "# ============================\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "split_documents = []\n",
    "for doc in all_documents:\n",
    "    chunks = text_splitter.split_text(doc.page_content)  # Split if needed\n",
    "    for chunk in chunks:\n",
    "        split_documents.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "\n",
    "print(f\"Total {len(split_documents)} final chunks prepared for vector storage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11067/1802458573.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, cache_folder = custom_cache_dir)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded successfully.\n",
      "Vector store exists. Deleting existing database...\n",
      "Vector store recreated and persisted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11067/1802458573.py:29: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================\n",
    "# 4. Create Embeddings\n",
    "# ============================\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, cache_folder = custom_cache_dir)\n",
    "print(\"Embeddings loaded successfully.\")\n",
    "\n",
    "# ============================\n",
    "# 5. Manage Vector Store\n",
    "# ============================\n",
    "persist_directory = \"chroma_db\"\n",
    "\n",
    "# Check if the vector store exists and delete it if necessary\n",
    "if RELOAD_VECTORS_DB:\n",
    "\n",
    "    if os.path.exists(persist_directory):\n",
    "        print(\"Vector store exists. Deleting existing database...\")\n",
    "        shutil.rmtree(persist_directory)  # Deletes the existing database folder\n",
    "\n",
    "    # Recreate the vector store\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=split_documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "else:\n",
    "    vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    print(\"Local Vector store loaded successfully.\")\n",
    "\n",
    "vectorstore.persist()\n",
    "print(\"Vector store recreated and persisted.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2-7B-Instruct; this may take some time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:54<00:00, 28.73s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================\n",
    "# 6. Set Up the LLM (Falcon 7B Instruct)\n",
    "# ============================\n",
    "# Load the tokenizer and model\n",
    "print(f\"Loading {LLM_MODEL_ID}; this may take some time...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID, trust_remote_code=True, cache_dir=custom_cache_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map= device,           # automatically place model layers on available GPU\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=custom_cache_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "/tmp/ipykernel_11067/908739846.py:14: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipeline_llm)\n"
     ]
    }
   ],
   "source": [
    "# Create a text-generation pipeline\n",
    "pipeline_llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=20,\n",
    "    temperature= 0.1,       # Lower temperature for more factual answers\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "# Wrap the pipeline in a LangChain LLM\n",
    "llm = HuggingFacePipeline(pipeline=pipeline_llm)\n",
    "\n",
    "\n",
    "# Customized Prompt\n",
    "\n",
    "QA_Prompt = \"\"\"\n",
    "You are an expert assistant answering factual questions about Pittsburgh or Carnegie Mellon University (CMU). \n",
    "Use the retrieved context to give a detailed and helpful answer. If the provided context does not contain the answer, leverage your pretraining knowledge to provide the correct answer. \n",
    "\n",
    "Important Instructions:\n",
    "- Answer concisely without repeating the question.\n",
    "- Use the provided context if relevant; otherwise, rely on your pretraining knowledge.\n",
    "- Do **not** use complete sentences. Provide only the word, name, date, or phrase that directly answers the question. For example, given the question \"When was Carnegie Mellon University founded?\", you should only answer \"1900\".\n",
    "\n",
    "Retrieved Context:\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "Examples:\n",
    "\n",
    "Question: In less than 5 words, Who is Pittsburgh named after? \n",
    "Answer: William Pitt \\n\n",
    "Question: In less than 5 words, What famous machine learning venue had its first conference in Pittsburgh in 1980? \n",
    "Answer: ICML \\n\n",
    "Question: In less than 5 words, What musical artist is performing at PPG Arena on October 13? \n",
    "Answer: Billie Eilish \\n\n",
    "\n",
    "Now it's your turn. Please answer the following question based on the above context. Remember to answer as short as possible. \n",
    "\n",
    "Question: In less than 5 words, {question} \\n\\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "custom_prompt = PromptTemplate(template=QA_Prompt, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 7. Create the RetrievalQA Chain\n",
    "# ============================\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": retriever_top_k})\n",
    "\n",
    "\n",
    "def ask_question(query: str):\n",
    "    \"\"\"\n",
    "    Run a query through the RAG pipeline and return the generated answer along with the source documents.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user’s question.\n",
    "\n",
    "    Returns:\n",
    "        answer (str): The generated answer.\n",
    "        sources (list): List of retrieved documents used to generate the answer.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    # print(f\"Retrieved {len(retrieved_docs)} documents.\")\n",
    "    \n",
    "    # Extract text from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    # print(f\"Context length: {len(context)} characters.\")\n",
    "    # print('------ START CONTEXT ------')\n",
    "    # print(context)\n",
    "    # print('------ END CONTEXT ------')\n",
    "\n",
    "    # Format the input using the QA_Prompt\n",
    "    formatted_prompt = QA_Prompt.format(context=context, question=query)\n",
    "    \n",
    "    # Generate response using the LLM\n",
    "    result = llm(formatted_prompt)  # Pass the fully formatted input\n",
    "    answer = result.replace(formatted_prompt, \"\").strip()\n",
    "    # Extract answer and sources\n",
    "    answer = answer.strip()  # Ensure clean output\n",
    "    return answer, retrieved_docs  # Return both answer and retrieved documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA_Prompt.format(context='d', question='2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/ubuntu/project/annotations/test_set.csv\", header=None)\n",
    "df.columns = ['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": retriever_top_k})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/575 [00:00<?, ?it/s]/tmp/ipykernel_11067/908739846.py:69: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n",
      "/tmp/ipykernel_11067/908739846.py:83: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = llm(formatted_prompt)  # Pass the fully formatted input\n",
      "  2%|▏         | 10/575 [00:07<05:41,  1.66it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 575/575 [05:53<00:00,  1.63it/s]\n",
      "100%|██████████| 575/575 [05:45<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in range(3,5):\n",
    "    questions = []\n",
    "\n",
    "    answers = []\n",
    "    sources = []\n",
    "    errors = []\n",
    "    full = df.shape[0]\n",
    "    subset = full\n",
    "\n",
    "    for i in trange(full):\n",
    "        row = df.iloc[i]\n",
    "\n",
    "        answer = \"I don't know.\"\n",
    "\n",
    "        try:\n",
    "            answer, retrieved_docs = ask_question(row['question'])\n",
    "        except:\n",
    "            errors.append((row['question']))\n",
    "            continue\n",
    "        # print(answer)\n",
    "        answer = answer.strip()\n",
    "        # print(answer)\n",
    "        answer = answer.split('\\n')[0]\n",
    "        answers.append(answer)\n",
    "        questions.append(row['question'])\n",
    "        sources.append(retrieved_docs)\n",
    "\n",
    "\n",
    "\n",
    "    df_ans = pd.DataFrame({'question': questions, 'answer': answers, 'source': sources})\n",
    "    df_ans.to_csv(f\"test_answers_adj_{k}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = 438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = df_ans.iloc[ind]['question']\n",
    "# answer = df_ans.iloc[ind]['answer']\n",
    "\n",
    "# print(question)\n",
    "# print(answer)\n",
    "# print('------')\n",
    "# for doc in df_ans.iloc[ind].source:\n",
    "#     print(doc.metadata['source'])\n",
    "#     print(doc.page_content)\n",
    "#     print('------')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 30})\n",
    "# docs = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc in docs:\n",
    "#     print(doc.metadata['source'])\n",
    "#     print(doc.page_content)\n",
    "#     print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans.to_csv(\"test_answers_adj_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
