{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/anlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# RAG_System.ipynb\n",
    "\n",
    "# ============================\n",
    "# 1. Install Required Packages\n",
    "# ============================\n",
    "# You might already have some or all of these. If so, you can skip or comment them out.\n",
    "# %pip install langchain transformers chromadb sentence-transformers accelerate bitsandbytes  # etc.\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "import shutil\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import gc\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"mps\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 2. Configuration\n",
    "# ============================\n",
    "# Path to data folder\n",
    "TEXT_DATA_PATH = [\"../data/zianp\", \"../data/dunhanj\"] \n",
    "ROW_EVENT_PATH = ['../data/nicolaw']\n",
    "STATIC_WEB_CSV_PATH = '../data/texts_urls_filtered.csv'\n",
    "custom_cache_dir = \"/mnt/new_volume\"\n",
    "\n",
    "# Choose an embedding model.\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_INPUT = \"qwq\"\n",
    "# Choose a local LLM model.\n",
    "# LLM_MODEL_ID = \"tiiuae/falcon-7b-instruct\"\n",
    "# LLM_MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "LLM_MODEL_MAP = {\"falcon\": \"tiiuae/falcon-7b-instruct\"\n",
    "                ,\"llama3\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "                ,\"deepseek-r1\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "                ,\"phi-4\": \"unsloth/phi-4-bnb-4bit\"\n",
    "                ,\"qwen2\": \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "                ,\"qwq\":\"Qwen/QwQ-32B\"}\n",
    "\n",
    "LLM_MODEL_ID = LLM_MODEL_MAP[LLM_INPUT]\n",
    "# LLM_MODEL_ID = \"unsloth/phi-4-bnb-4bit\"\n",
    "\n",
    "LLM_NAME = LLM_MODEL_ID.split(\"/\")[-1]\n",
    "data_file = \"qa400\"\n",
    "test_data_path =\"../annotations/{}.csv\".format(data_file)\n",
    "\n",
    "retriever_top_k = 4\n",
    "CHUNK_SIZE = 512  \n",
    "CHUNK_OVERLAP = 100\n",
    "RELOAD_VECTORS_DB = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify files in the folder\n",
    "\n",
    "files_txt_path = []\n",
    "files_csv_path = []\n",
    "files_event_path = []\n",
    "\n",
    "for DATA_PATH in TEXT_DATA_PATH:\n",
    "    for root, dirs, files in os.walk(DATA_PATH):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                files_txt_path.append(os.path.join(root, file))\n",
    "            elif file.endswith('.csv'):\n",
    "                files_csv_path.append(os.path.join(root, file))\n",
    "\n",
    "for DATA_PATH in ROW_EVENT_PATH:\n",
    "    for root, dirs, files in os.walk(DATA_PATH):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                files_event_path.append(os.path.join(root, file))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8540 raw documents from 13 files.\n",
      "Total 99184 final chunks prepared for vector storage.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================\n",
    "# 2. Load Files with Different Strategies\n",
    "# ============================\n",
    "all_documents = []\n",
    "\n",
    "# Load Dunhan CSV\n",
    "test_df = pd.read_csv(STATIC_WEB_CSV_PATH)\n",
    "for index, row in test_df.iterrows():\n",
    "    \n",
    "    all_documents.append(Document(page_content=row['TEXT'], metadata={\"source\": row['URL']}))\n",
    "\n",
    "# Load all files in the directory\n",
    "for file_path in files_txt_path:\n",
    "    loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "    doc = loader.load()  # Load entire file as one document\n",
    "    all_documents.append(Document(page_content=doc[0].page_content, metadata={\"source\": file_path}))\n",
    "\n",
    "for file_path in files_csv_path:\n",
    "    df = pd.read_csv(file_path)\n",
    "    filename = os.path.basename(file_path)\n",
    "    for index, row in df.iterrows():\n",
    "        row_text = f\"{filename} | \" + \" | \".join(f\"{col}: {row[col]}\" for col in df.columns)\n",
    "        metadata = {\"source\": filename, \"row_id\": index}\n",
    "        all_documents.append(Document(page_content=row_text, metadata=metadata))\n",
    "\n",
    "\n",
    "# OPTIOANL function for processing files row by row\n",
    "    # âœ… Load row by row (structured data)\n",
    "for file_path in files_event_path:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for row_id, line in enumerate(file):\n",
    "            line = line.strip()\n",
    "            if line:  # Ignore empty lines\n",
    "                all_documents.append(Document(page_content=line, metadata={\"source\": filename, \"row_id\": row_id}))\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(all_documents)} raw documents from {len(os.listdir(DATA_PATH))} files.\")\n",
    "\n",
    "# ============================\n",
    "# 3. Split Longer Documents for Better Retrieval\n",
    "# ============================\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "split_documents = []\n",
    "for doc in all_documents:\n",
    "    chunks = text_splitter.split_text(doc.page_content)  # Split if needed\n",
    "    for chunk in chunks:\n",
    "        split_documents.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "\n",
    "print(f\"Total {len(split_documents)} final chunks prepared for vector storage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13764/1802458573.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, cache_folder = custom_cache_dir)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13764/1802458573.py:26: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Vector store loaded successfully.\n",
      "Vector store recreated and persisted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13764/1802458573.py:29: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================\n",
    "# 4. Create Embeddings\n",
    "# ============================\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, cache_folder = custom_cache_dir)\n",
    "print(\"Embeddings loaded successfully.\")\n",
    "\n",
    "# ============================\n",
    "# 5. Manage Vector Store\n",
    "# ============================\n",
    "persist_directory = \"chroma_db\"\n",
    "\n",
    "# Check if the vector store exists and delete it if necessary\n",
    "if RELOAD_VECTORS_DB:\n",
    "\n",
    "    if os.path.exists(persist_directory):\n",
    "        print(\"Vector store exists. Deleting existing database...\")\n",
    "        shutil.rmtree(persist_directory)  # Deletes the existing database folder\n",
    "\n",
    "    # Recreate the vector store\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=split_documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "else:\n",
    "    vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    print(\"Local Vector store loaded successfully.\")\n",
    "\n",
    "vectorstore.persist()\n",
    "print(\"Vector store recreated and persisted.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta-llama/Llama-3.1-8B-Instruct; this may take some time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:01<00:00, 30.39s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================\n",
    "# 6. Set Up the LLM (Falcon 7B Instruct)\n",
    "# ============================\n",
    "# Load the tokenizer and model\n",
    "print(f\"Loading {LLM_MODEL_ID}; this may take some time...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID, trust_remote_code=True, cache_dir=custom_cache_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map= device,           # automatically place model layers on available GPU\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=custom_cache_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "/tmp/ipykernel_13764/908739846.py:14: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipeline_llm)\n"
     ]
    }
   ],
   "source": [
    "# Create a text-generation pipeline\n",
    "pipeline_llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=20,\n",
    "    temperature= 0.1,       # Lower temperature for more factual answers\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "# Wrap the pipeline in a LangChain LLM\n",
    "llm = HuggingFacePipeline(pipeline=pipeline_llm)\n",
    "\n",
    "\n",
    "# Customized Prompt\n",
    "\n",
    "QA_Prompt = \"\"\"\n",
    "You are an expert assistant answering factual questions about Pittsburgh or Carnegie Mellon University (CMU). \n",
    "Use the retrieved context to give a detailed and helpful answer. If the provided context does not contain the answer, leverage your pretraining knowledge to provide the correct answer. \n",
    "\n",
    "Important Instructions:\n",
    "- Answer concisely without repeating the question.\n",
    "- Use the provided context if relevant; otherwise, rely on your pretraining knowledge.\n",
    "- Do **not** use complete sentences. Provide only the word, name, date, or phrase that directly answers the question. For example, given the question \"When was Carnegie Mellon University founded?\", you should only answer \"1900\".\n",
    "\n",
    "Retrieved Context:\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "Examples:\n",
    "\n",
    "Question: In less than 5 words, Who is Pittsburgh named after? \n",
    "Answer: William Pitt \\n\n",
    "Question: In less than 5 words, What famous machine learning venue had its first conference in Pittsburgh in 1980? \n",
    "Answer: ICML \\n\n",
    "Question: In less than 5 words, What musical artist is performing at PPG Arena on October 13? \n",
    "Answer: Billie Eilish \\n\n",
    "\n",
    "Now it's your turn. Please answer the following question based on the above context. Remember to answer as short as possible. \n",
    "\n",
    "Question: In less than 5 words, {question} \\n\\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "custom_prompt = PromptTemplate(template=QA_Prompt, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 7. Create the RetrievalQA Chain\n",
    "# ============================\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": retriever_top_k})\n",
    "\n",
    "\n",
    "def ask_question(query: str):\n",
    "    \"\"\"\n",
    "    Run a query through the RAG pipeline and return the generated answer along with the source documents.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The userâ€™s question.\n",
    "\n",
    "    Returns:\n",
    "        answer (str): The generated answer.\n",
    "        sources (list): List of retrieved documents used to generate the answer.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    # print(f\"Retrieved {len(retrieved_docs)} documents.\")\n",
    "    \n",
    "    # Extract text from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    # print(f\"Context length: {len(context)} characters.\")\n",
    "    # print('------ START CONTEXT ------')\n",
    "    # print(context)\n",
    "    # print('------ END CONTEXT ------')\n",
    "\n",
    "    # Format the input using the QA_Prompt\n",
    "    formatted_prompt = QA_Prompt.format(context=context, question=query)\n",
    "    \n",
    "    # Generate response using the LLM\n",
    "    result = llm(formatted_prompt)  # Pass the fully formatted input\n",
    "    answer = result.replace(formatted_prompt, \"\").strip()\n",
    "    # Extract answer and sources\n",
    "    answer = answer.strip()  # Ensure clean output\n",
    "    return answer, retrieved_docs  # Return both answer and retrieved documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA_Prompt.format(context='d', question='2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_13764/908739846.py:69: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n",
      "/tmp/ipykernel_13764/908739846.py:83: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = llm(formatted_prompt)  # Pass the fully formatted input\n",
      "  5%|â–Œ         | 1/20 [00:03<01:02,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "William Pitt.  Source: 1758 letter to Pitt.  Named by General John Forbes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 2/20 [00:04<00:34,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1900 | 1967 (merger) | 1913 (partially related institute)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 3/20 [00:04<00:21,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smithfield Street Bridge.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:18,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luis von Ahn  | No mention of this information in the text. However, I can tell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:13,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matt Light.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:06<00:10,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downtown Cultural District.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:06<00:07,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kevin McMahon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:07<00:08,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Giving Team  --- OR ---   Planned Giving Team  --- OR ---   Legacy Society Team\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:08<00:06,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please contact us.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:09<00:07,  1.35it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gift Illustrator  -OR-  Planner Library  -OR-  Gift Illustrator Your Planning Library\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:09<00:06,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steelers Hall of Honor Museum.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:10<00:04,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acrisure Stadium.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:11<00:04,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberto Clemente  --- end of response --- \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- end of conversation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:11<00:03,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penn Brewery.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:11<00:02,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pittsburgh Neighborhood Trail.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:12<00:02,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congregation Beth Shalom.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:13<00:01,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None mentioned.  However, I can tell you that there isn't any information regarding the hotel partners\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:14<00:01,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No information available in this context. However, according to my training data, the founder of the Pittsburgh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:14<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Nosh \n",
      "\n",
      "Let me know if I'm right!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jewish Federation of Greater Pittsburgh.  The Steel Tree Fund.  OR   Both options are acceptable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "sources = []\n",
    "\n",
    "full = df.shape[0]\n",
    "subset = 20\n",
    "for i in trange(subset):\n",
    "    row = df.iloc[i]\n",
    "    answer, retrieved_docs = ask_question(row['question'])\n",
    "    print(answer)\n",
    "    answer = answer.split('\\n')[0]\n",
    "    answers.append(answer)\n",
    "    sources.append(retrieved_docs)\n",
    "\n",
    "df_ans = pd.DataFrame({'question': df['question'][:subset], 'answer': answers, 'reference_answer': df['reference_answer'][:subset], 'source': sources})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans.to_csv(f'../results/test_{data_file}_{LLM_NAME}_ck{CHUNK_SIZE}_ckolap{CHUNK_OVERLAP}_retop{retriever_top_k}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ans.to_csv('../results/test_30.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the total expenditure forecast for the City of Pittsburgh in 2024?\n",
      "Generated Answer: $ 1,097,536,446\n",
      "Reference Answer: 684,553,037\n",
      "[Source 1] ../data/dunhanj/2024_operating_budget.txt\n",
      "[Source 2] ../data/dunhanj/2024_operating_budget.txt\n",
      "[Source 3] ../data/dunhanj/2024_operating_budget.txt\n",
      "[Source 4] ../data/dunhanj/2024_operating_budget.txt\n",
      "\n",
      "\n",
      "\n",
      "Question: Which department has the highest budget allocation in 2024?\n",
      "Generated Answer: Department of Law\n",
      "Reference Answer: Finance, with a budget of $190,821,098.\n",
      "[Source 1] ../data/dunhanj/2024_operating_budget.txt\n",
      "[Source 2] ../data/dunhanj/2024_operating_budget.txt\n",
      "[Source 3] ../data/dunhanj/2024_operating_budget.txt\n",
      "[Source 4] ../data/dunhanj/2024_operating_budget.txt\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Example:\n",
    "# user_question = \"In few words, what time will Kimberly Akimbo take place?\"\n",
    "# user_question = \"Which bridge should drivers use as an alternate route to avoid congestion at I-279 Northbound Exit 1B on event days?\"\n",
    "\n",
    "# #\"question\": \"What is the total expenditure forecast for the City of Pittsburgh in 2024?\",\n",
    "# # \"answer\": \"$684,553,037\"\n",
    "# question_list = [(\"What is the total expenditure forecast for the City of Pittsburgh in 2024?\",\"684,553,037\")\n",
    "#                  ,(\"Which department has the highest budget allocation in 2024?\",\"Finance, with a budget of $190,821,098.\")]\n",
    "# # question_list = [('When was Carnegie Technical Schools founded?', '1900')]\n",
    "\n",
    "# for question, ref_ans in question_list:\n",
    "#     user_question = question\n",
    "#     print(\"Question:\", user_question)\n",
    "#     answer, sources = ask_question(user_question)\n",
    "#     print(\"Generated Answer:\", answer)\n",
    "#     print(\"Reference Answer:\", ref_ans)\n",
    "\n",
    "#     for i, doc in enumerate(sources):\n",
    "#         print(f\"[Source {i+1}] {doc.metadata.get('source', 'Unknown source')}\")\n",
    "    \n",
    "#     print(\"\\n\\n\")\n",
    "\n",
    "# # print(\"Question:\", user_question)\n",
    "# # print(answer)\n",
    "# # print(\"\\nSources used:\")\n",
    "# # for i, doc in enumerate(sources):\n",
    "# #     print(f\"[Source {i+1}] {doc.metadata.get('source', 'Unknown source')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.read_csv('../results/test_1000_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is Pittsburgh named after?</td>\n",
       "      <td>William Pitt</td>\n",
       "      <td>William Pitt</td>\n",
       "      <td>[Document(metadata={'source': 'https://web.arc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What year was Carnegie Mellon University founded?</td>\n",
       "      <td>&lt;|repo_name|&gt;jamesr66/qa-pittsburgh-cmu&lt;|file</td>\n",
       "      <td>1900</td>\n",
       "      <td>[Document(metadata={'source': '../data/zianp/w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which bridge in Pittsburgh is famously yellow?</td>\n",
       "      <td>Fort Duquesne Bridge</td>\n",
       "      <td>Roberto Clemente Bridge</td>\n",
       "      <td>[Document(metadata={'source': 'https://trustar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which famous AI professor at CMU co-founded Du...</td>\n",
       "      <td>Luis von Ahn</td>\n",
       "      <td>Luis von Ahn</td>\n",
       "      <td>[Document(metadata={'source': 'https://www.cmu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who hosts the Burgh Bus comedy tour in Pittsbu...</td>\n",
       "      <td>Matt Light</td>\n",
       "      <td>Matt Light.</td>\n",
       "      <td>[Document(metadata={'source': 'https://downtow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>What is the name of the city that hosts the Ci...</td>\n",
       "      <td>Western Pennsylvania## Question ##</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>[Document(metadata={'source': '../data/zianp/w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>What is the name of the city that is home to t...</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>[Document(metadata={'source': '../data/zianp/w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>What is the name of the famous university in P...</td>\n",
       "      <td>Carnegie Mellon University</td>\n",
       "      <td>Carnegie Mellon University</td>\n",
       "      <td>[Document(metadata={'source': 'https://kids.br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>What is the name of the famous comedy tour in ...</td>\n",
       "      <td>Pittsburgh Improv Theatre</td>\n",
       "      <td>Burgh Bus</td>\n",
       "      <td>[Document(metadata={'source': 'https://downtow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>Who is the famous AI professor at Carnegie Mel...</td>\n",
       "      <td>Luis von Ahn</td>\n",
       "      <td>Luis von Ahn</td>\n",
       "      <td>[Document(metadata={'source': '../data/zianp/w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>435 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0                       Who is Pittsburgh named after?   \n",
       "1    What year was Carnegie Mellon University founded?   \n",
       "2       Which bridge in Pittsburgh is famously yellow?   \n",
       "3    Which famous AI professor at CMU co-founded Du...   \n",
       "4    Who hosts the Burgh Bus comedy tour in Pittsbu...   \n",
       "..                                                 ...   \n",
       "430  What is the name of the city that hosts the Ci...   \n",
       "431  What is the name of the city that is home to t...   \n",
       "432  What is the name of the famous university in P...   \n",
       "433  What is the name of the famous comedy tour in ...   \n",
       "434  Who is the famous AI professor at Carnegie Mel...   \n",
       "\n",
       "                                            answer  \\\n",
       "0                                     William Pitt   \n",
       "1    <|repo_name|>jamesr66/qa-pittsburgh-cmu<|file   \n",
       "2                             Fort Duquesne Bridge   \n",
       "3                                     Luis von Ahn   \n",
       "4                                       Matt Light   \n",
       "..                                             ...   \n",
       "430             Western Pennsylvania## Question ##   \n",
       "431                                     Pittsburgh   \n",
       "432                     Carnegie Mellon University   \n",
       "433                      Pittsburgh Improv Theatre   \n",
       "434                                   Luis von Ahn   \n",
       "\n",
       "               reference_answer  \\\n",
       "0                  William Pitt   \n",
       "1                          1900   \n",
       "2       Roberto Clemente Bridge   \n",
       "3                  Luis von Ahn   \n",
       "4                   Matt Light.   \n",
       "..                          ...   \n",
       "430                  Pittsburgh   \n",
       "431                  Pittsburgh   \n",
       "432  Carnegie Mellon University   \n",
       "433                   Burgh Bus   \n",
       "434                Luis von Ahn   \n",
       "\n",
       "                                                source  \n",
       "0    [Document(metadata={'source': 'https://web.arc...  \n",
       "1    [Document(metadata={'source': '../data/zianp/w...  \n",
       "2    [Document(metadata={'source': 'https://trustar...  \n",
       "3    [Document(metadata={'source': 'https://www.cmu...  \n",
       "4    [Document(metadata={'source': 'https://downtow...  \n",
       "..                                                 ...  \n",
       "430  [Document(metadata={'source': '../data/zianp/w...  \n",
       "431  [Document(metadata={'source': '../data/zianp/w...  \n",
       "432  [Document(metadata={'source': 'https://kids.br...  \n",
       "433  [Document(metadata={'source': 'https://downtow...  \n",
       "434  [Document(metadata={'source': '../data/zianp/w...  \n",
       "\n",
       "[435 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
