{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://en.wikipedia.org/wiki/Pittsburgh (Depth 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gz/yrdfvq7156ggy1rjy5rj56gh0000gn/T/ipykernel_54285/2918151965.py:42: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]  # Read table into DataFrame\n",
      "/var/folders/gz/yrdfvq7156ggy1rjy5rj56gh0000gn/T/ipykernel_54285/2918151965.py:42: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]  # Read table into DataFrame\n",
      "/var/folders/gz/yrdfvq7156ggy1rjy5rj56gh0000gn/T/ipykernel_54285/2918151965.py:42: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]  # Read table into DataFrame\n",
      "/var/folders/gz/yrdfvq7156ggy1rjy5rj56gh0000gn/T/ipykernel_54285/2918151965.py:42: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]  # Read table into DataFrame\n",
      "/var/folders/gz/yrdfvq7156ggy1rjy5rj56gh0000gn/T/ipykernel_54285/2918151965.py:42: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]  # Read table into DataFrame\n",
      "/var/folders/gz/yrdfvq7156ggy1rjy5rj56gh0000gn/T/ipykernel_54285/2918151965.py:42: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]  # Read table into DataFrame\n",
      "/var/folders/gz/yrdfvq7156ggy1rjy5rj56gh0000gn/T/ipykernel_54285/2918151965.py:42: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]  # Read table into DataFrame\n",
      "/var/folders/gz/yrdfvq7156ggy1rjy5rj56gh0000gn/T/ipykernel_54285/2918151965.py:42: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]  # Read table into DataFrame\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: wiki_pages/Pittsburgh.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "WIKI_BASE_URL = \"https://en.wikipedia.org\"\n",
    "OUTPUT_DIR = \"../data/wiki/pittsburgh/\"\n",
    "\n",
    "# Create output directory if not exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def get_wikipedia_content(url):\n",
    "    \"\"\"Fetch and extract main text and tables from a Wikipedia page.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return None, None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract page title\n",
    "    title = soup.find(\"h1\", {\"id\": \"firstHeading\"}).text.strip()\n",
    "\n",
    "    # Extract main text content\n",
    "    content_div = soup.find(\"div\", {\"id\": \"bodyContent\"})\n",
    "    if not content_div:\n",
    "        return title, None\n",
    "\n",
    "    paragraphs = content_div.find_all(\"p\", recursive=True)\n",
    "    text = \"\\n\".join(p.get_text() for p in paragraphs if p.get_text().strip())\n",
    "\n",
    "    # Remove reference markers like [1], [2], etc.\n",
    "    cleaned_text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "\n",
    "    # Extract tables\n",
    "    tables = soup.find_all(\"table\", {\"class\": \"wikitable\"})\n",
    "    table_texts = []\n",
    "\n",
    "    for i, table in enumerate(tables):\n",
    "        df = pd.read_html(str(table))[0]  # Read table into DataFrame\n",
    "        table_filename = os.path.join(OUTPUT_DIR, f\"{title}_table_{i+1}.csv\")\n",
    "        df.to_csv(table_filename, index=False)  # Save table as CSV\n",
    "        # table_texts.append(f\"\\n[Table {i+1} saved as {table_filename}]\\n\")\n",
    "\n",
    "    # Append table info to content\n",
    "    final_content = cleaned_text + \"\\n\\n\" + \"\\n\".join(table_texts)\n",
    "    \n",
    "    return title, final_content\n",
    "\n",
    "def save_content(title, content):\n",
    "    \"\"\"Save Wikipedia content to a text file.\"\"\"\n",
    "    if not content:\n",
    "        return\n",
    "    \n",
    "    filename = re.sub(r'[\\\\/*?:\"<>|]', \"\", title) + \".txt\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "    \n",
    "    print(f\"Saved: {filepath}\")\n",
    "\n",
    "def find_wiki_links(url, max_links=5):\n",
    "    \"\"\"Find internal Wikipedia links from a given page.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = set()\n",
    "\n",
    "    for link in soup.select(\"div#bodyContent a[href^='/wiki/']\"):\n",
    "        full_url = urljoin(WIKI_BASE_URL, link['href'])\n",
    "        if \":\" not in link['href']:  # Avoid special pages\n",
    "            links.add(full_url)\n",
    "        if len(links) >= max_links:\n",
    "            break\n",
    "\n",
    "    return list(links)\n",
    "\n",
    "def crawl_wikipedia(start_url, depth=2):\n",
    "    \"\"\"Crawl Wikipedia starting from `start_url` up to a given depth.\"\"\"\n",
    "    visited = set()\n",
    "    to_visit = [(start_url, 0)]\n",
    "\n",
    "    while to_visit:\n",
    "        url, level = to_visit.pop(0)\n",
    "        if url in visited or level > depth:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Crawling: {url} (Depth {level})\")\n",
    "        visited.add(url)\n",
    "\n",
    "        title, content = get_wikipedia_content(url)\n",
    "        if content:\n",
    "            save_content(title, content)\n",
    "\n",
    "        if level < depth:\n",
    "            new_links = find_wiki_links(url)\n",
    "            to_visit.extend((link, level + 1) for link in new_links)\n",
    "\n",
    "        time.sleep(1)  # Be polite to Wikipedia servers\n",
    "\n",
    "# Example usage: Crawl Wikipedia starting from the \"Pittsburgh\" page\n",
    "start_url = \"https://en.wikipedia.org/wiki/Pittsburgh\"\n",
    "crawl_wikipedia(start_url, depth=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"wiki_pages/Pittsburgh_table_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Race / Ethnicity (NH = Non-Hispanic)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Pop 1980[110]",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Pop 1990[111]",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Pop 2000[112]",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Pop 2010[113]",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Pop 2020[114]",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "% 1980",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "% 1990",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "% 2000",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "% 2010",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "% 2020",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "95b71867-9fc3-4636-a902-5a100697a2cc",
       "rows": [
        [
         "0",
         "White alone (NH)",
         "316262.0",
         "264722.0",
         "223982",
         "198186",
         "187099",
         "74.60%",
         "71.57%",
         "66.95%",
         "64.83%",
         "61.75%"
        ],
        [
         "1",
         "Black or African American alone (NH)",
         "100734.0",
         "94743.0",
         "90183",
         "78847",
         "68314",
         "23.76%",
         "25.61%",
         "26.96%",
         "25.79%",
         "22.55%"
        ],
        [
         "2",
         "Native American or Alaska Native alone (NH)",
         "552.0",
         "583.0",
         "561",
         "505",
         "475",
         "0.13%",
         "0.16%",
         "0.17%",
         "0.17%",
         "0.16%"
        ],
        [
         "3",
         "Asian alone (NH)",
         "2778.0",
         "5865.0",
         "9160",
         "13393",
         "19745",
         "0.66%",
         "1.59%",
         "2.74%",
         "4.38%",
         "6.52%"
        ],
        [
         "4",
         "Pacific Islander alone (NH)",
         null,
         null,
         "100",
         "76",
         "96",
         null,
         null,
         "0.03%",
         "0.02%",
         "0.03%"
        ],
        [
         "5",
         "Other race alone (NH)",
         "242.0",
         "498.0",
         "1217",
         "843",
         "2081",
         "0.06%",
         "0.13%",
         "0.36%",
         "0.28%",
         "0.69%"
        ],
        [
         "6",
         "Mixed race or Multiracial (NH)",
         null,
         null,
         "4935",
         "6890",
         "13541",
         null,
         null,
         "1.48%",
         "2.25%",
         "4.47%"
        ],
        [
         "7",
         "Hispanic or Latino (any race)",
         "3370.0",
         "3468.0",
         "4425",
         "6964",
         "11620",
         "0.79%",
         "0.94%",
         "1.32%",
         "2.28%",
         "3.84%"
        ],
        [
         "8",
         "Total",
         "423938.0",
         "369879.0",
         "334563",
         "305704",
         "302971",
         "100.00%",
         "100.00%",
         "100.00%",
         "100.00%",
         "100.00%"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 9
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Race / Ethnicity (NH = Non-Hispanic)</th>\n",
       "      <th>Pop 1980[110]</th>\n",
       "      <th>Pop 1990[111]</th>\n",
       "      <th>Pop 2000[112]</th>\n",
       "      <th>Pop 2010[113]</th>\n",
       "      <th>Pop 2020[114]</th>\n",
       "      <th>% 1980</th>\n",
       "      <th>% 1990</th>\n",
       "      <th>% 2000</th>\n",
       "      <th>% 2010</th>\n",
       "      <th>% 2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>White alone (NH)</td>\n",
       "      <td>316262.0</td>\n",
       "      <td>264722.0</td>\n",
       "      <td>223982</td>\n",
       "      <td>198186</td>\n",
       "      <td>187099</td>\n",
       "      <td>74.60%</td>\n",
       "      <td>71.57%</td>\n",
       "      <td>66.95%</td>\n",
       "      <td>64.83%</td>\n",
       "      <td>61.75%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Black or African American alone (NH)</td>\n",
       "      <td>100734.0</td>\n",
       "      <td>94743.0</td>\n",
       "      <td>90183</td>\n",
       "      <td>78847</td>\n",
       "      <td>68314</td>\n",
       "      <td>23.76%</td>\n",
       "      <td>25.61%</td>\n",
       "      <td>26.96%</td>\n",
       "      <td>25.79%</td>\n",
       "      <td>22.55%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Native American or Alaska Native alone (NH)</td>\n",
       "      <td>552.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>561</td>\n",
       "      <td>505</td>\n",
       "      <td>475</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>0.16%</td>\n",
       "      <td>0.17%</td>\n",
       "      <td>0.17%</td>\n",
       "      <td>0.16%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Asian alone (NH)</td>\n",
       "      <td>2778.0</td>\n",
       "      <td>5865.0</td>\n",
       "      <td>9160</td>\n",
       "      <td>13393</td>\n",
       "      <td>19745</td>\n",
       "      <td>0.66%</td>\n",
       "      <td>1.59%</td>\n",
       "      <td>2.74%</td>\n",
       "      <td>4.38%</td>\n",
       "      <td>6.52%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pacific Islander alone (NH)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>76</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03%</td>\n",
       "      <td>0.02%</td>\n",
       "      <td>0.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Other race alone (NH)</td>\n",
       "      <td>242.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>1217</td>\n",
       "      <td>843</td>\n",
       "      <td>2081</td>\n",
       "      <td>0.06%</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>0.36%</td>\n",
       "      <td>0.28%</td>\n",
       "      <td>0.69%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mixed race or Multiracial (NH)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4935</td>\n",
       "      <td>6890</td>\n",
       "      <td>13541</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.48%</td>\n",
       "      <td>2.25%</td>\n",
       "      <td>4.47%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hispanic or Latino (any race)</td>\n",
       "      <td>3370.0</td>\n",
       "      <td>3468.0</td>\n",
       "      <td>4425</td>\n",
       "      <td>6964</td>\n",
       "      <td>11620</td>\n",
       "      <td>0.79%</td>\n",
       "      <td>0.94%</td>\n",
       "      <td>1.32%</td>\n",
       "      <td>2.28%</td>\n",
       "      <td>3.84%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Total</td>\n",
       "      <td>423938.0</td>\n",
       "      <td>369879.0</td>\n",
       "      <td>334563</td>\n",
       "      <td>305704</td>\n",
       "      <td>302971</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Race / Ethnicity (NH = Non-Hispanic)  Pop 1980[110]  Pop 1990[111]  \\\n",
       "0                             White alone (NH)       316262.0       264722.0   \n",
       "1         Black or African American alone (NH)       100734.0        94743.0   \n",
       "2  Native American or Alaska Native alone (NH)          552.0          583.0   \n",
       "3                             Asian alone (NH)         2778.0         5865.0   \n",
       "4                  Pacific Islander alone (NH)            NaN            NaN   \n",
       "5                        Other race alone (NH)          242.0          498.0   \n",
       "6               Mixed race or Multiracial (NH)            NaN            NaN   \n",
       "7                Hispanic or Latino (any race)         3370.0         3468.0   \n",
       "8                                        Total       423938.0       369879.0   \n",
       "\n",
       "   Pop 2000[112]  Pop 2010[113]  Pop 2020[114]   % 1980   % 1990   % 2000  \\\n",
       "0         223982         198186         187099   74.60%   71.57%   66.95%   \n",
       "1          90183          78847          68314   23.76%   25.61%   26.96%   \n",
       "2            561            505            475    0.13%    0.16%    0.17%   \n",
       "3           9160          13393          19745    0.66%    1.59%    2.74%   \n",
       "4            100             76             96      NaN      NaN    0.03%   \n",
       "5           1217            843           2081    0.06%    0.13%    0.36%   \n",
       "6           4935           6890          13541      NaN      NaN    1.48%   \n",
       "7           4425           6964          11620    0.79%    0.94%    1.32%   \n",
       "8         334563         305704         302971  100.00%  100.00%  100.00%   \n",
       "\n",
       "    % 2010   % 2020  \n",
       "0   64.83%   61.75%  \n",
       "1   25.79%   22.55%  \n",
       "2    0.17%    0.16%  \n",
       "3    4.38%    6.52%  \n",
       "4    0.02%    0.03%  \n",
       "5    0.28%    0.69%  \n",
       "6    2.25%    4.47%  \n",
       "7    2.28%    3.84%  \n",
       "8  100.00%  100.00%  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp-hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
